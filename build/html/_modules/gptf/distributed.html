<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>gptf.distributed &mdash; gptf 1.0.0 documentation</title>
    
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="gptf 1.0.0 documentation" href="../../index.html" />
    <link rel="up" title="Module code" href="../index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for gptf.distributed</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- encoding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;Provides classes for Robust Bayesian Committee Machines.&quot;&quot;&quot;</span>
<span class="c1">#TODO: Work out the maths for a non-zero prior mean function?</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">super</span><span class="p">,</span> <span class="nb">zip</span><span class="p">,</span> <span class="nb">map</span><span class="p">,</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">future.utils</span> <span class="kn">import</span> <span class="n">with_metaclass</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Sequence</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Sequence</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">zip_longest</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">izip_longest</span> <span class="k">as</span> <span class="n">zip_longest</span>

<span class="kn">from</span> <span class="nn">overrides</span> <span class="kn">import</span> <span class="n">overrides</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">gptf</span> <span class="kn">import</span> <span class="n">GPModel</span><span class="p">,</span> <span class="n">Parameterized</span><span class="p">,</span> <span class="n">ParamList</span><span class="p">,</span> <span class="n">ParamAttributes</span><span class="p">,</span> <span class="n">tf_method</span>

<div class="viewcode-block" id="cao_fleet_weights"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.cao_fleet_weights">[docs]</a><span class="k">def</span> <span class="nf">cao_fleet_weights</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The predictive power of the experts at the points.</span>

<span class="sd">    The predictive power is calculated as</span>

<span class="sd">    .. math::</span>
<span class="sd">        β_k(x_\star)=\\frac{1}{2} (\ln (σ^\star_k)^2 - \ln σ^{-2}_k(x_\star))</span>

<span class="sd">    where :math:`β_k(x_\star)` is the predictive power of the</span>
<span class="sd">    :math:`k`th expert at the point :math:`x_\star`, </span>
<span class="sd">    :math:`(σ^\star_k)^2` is the prior variance of the :math:`k`\ th</span>
<span class="sd">    expert and :math:`σ^{-2}_k(x_\star)` is the posterior variance</span>
<span class="sd">    of the :math:`k`\ th expert at the point :math:`x_\star`.</span>

<span class="sd">    Args:</span>
<span class="sd">        experts (Sequence[GPModel]): the experts to calculate</span>
<span class="sd">            the weights of.</span>
<span class="sd">        X (tf.Tensor): the training inputs, shape `[n, point_dims]`.</span>
<span class="sd">        Y (tf.Tensor): the training outputs, shape `[n, num_latent]`.</span>
<span class="sd">        points (tf.Tensor): the points at which to calculate the </span>
<span class="sd">            weights of the model, shape `[m, point_dims]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tuple[tf.Tensor]): The weights of the experts at the</span>
<span class="sd">        points. Each tensor has shape `(num_points, num_latent)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="n">expert</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">num_latent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prior_var</span> <span class="o">=</span> <span class="n">expert</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">post_var</span> <span class="o">=</span> <span class="n">expert</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">points</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prior_var</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">post_var</span><span class="p">))</span>
        <span class="c1"># return very small value instead of zero</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">experts</span><span class="p">)</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">weight</span><span class="p">(</span><span class="o">*</span><span class="n">chunk</span><span class="p">)</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">)</span></div>

<div class="viewcode-block" id="equal_weights"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.equal_weights">[docs]</a><span class="k">def</span> <span class="nf">equal_weights</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gives each expert an equal weight.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \\forall k \in 0..M,\ β_k = 1 / M</span>

<span class="sd">    where :math:`β_k` is the weight of the :math:`k`\ th expert at</span>
<span class="sd">    every point and :math:`M` is the number of experts.</span>

<span class="sd">    The dtype returned matches the dtype of `points`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        experts (Sequence[GPModel]): the experts to calculate</span>
<span class="sd">            the weights of.</span>
<span class="sd">        X (tf.Tensor): the training inputs, shape `[n, point_dims]`.</span>
<span class="sd">        Y (tf.Tensor): the training outputs, shape `[n, num_latent]`.</span>
<span class="sd">        points (tf.Tensor): the points at which to calculate the </span>
<span class="sd">            weights of the model, shape `[m, point_dims]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tuple[tf.Tensor]): The weights of the experts at the</span>
<span class="sd">        points. Each tensor has shape `(num_points, num_latent)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">experts</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">points</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">points</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">beta</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">experts</span><span class="p">)</span></div>

<div class="viewcode-block" id="ones_weights"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.ones_weights">[docs]</a><span class="k">def</span> <span class="nf">ones_weights</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;All weights are 1.</span>

<span class="sd">    The dtype returned matches the dtype of `points`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        experts (Sequence[GPModel]): the experts to calculate</span>
<span class="sd">            the weights of.</span>
<span class="sd">        X (tf.Tensor): the training inputs, shape `[n, point_dims]`.</span>
<span class="sd">        Y (tf.Tensor): the training outputs, shape `[n, num_latent]`.</span>
<span class="sd">        points (tf.Tensor): the points at which to calculate the </span>
<span class="sd">            weights of the model, shape `[m, point_dims]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tuple[tf.Tensor]): The weights of the experts at the</span>
<span class="sd">        points. Each tensor has shape `(num_points, num_latent)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">points</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">points</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ones</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">experts</span><span class="p">)</span></div>

<div class="viewcode-block" id="Reduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.Reduction">[docs]</a><span class="k">class</span> <span class="nc">Reduction</span><span class="p">(</span><span class="n">GPModel</span><span class="p">,</span> <span class="n">ParamList</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Common code for distributed GP reductions.&quot;&quot;&quot;</span>
    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="Reduction.build_log_likelihood"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.Reduction.build_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">build_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The sum of the log likelihoods of the children.&quot;&quot;&quot;</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_chunks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">lmls</span> <span class="o">=</span> <span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_log_likelihood</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span><span class="p">)</span> <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">lmls</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span></div>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="Reduction.build_prior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.Reduction.build_prior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_prior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The arithetic mean of the prior mean / variance of the chilren.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span>
                            <span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">,</span> <span class="n">full_cov</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">])</span>
        <span class="n">M</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">))</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">M</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">M</span></div>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_get_chunks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Seperates X and Y into chunks, pairing each chunk with a child.&quot;&quot;&quot;</span>
        <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span></div>

<div class="viewcode-block" id="PoEReduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PoEReduction">[docs]</a><span class="k">class</span> <span class="nc">PoEReduction</span><span class="p">(</span><span class="n">Reduction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the predictions of its children using the PoE model.</span>
<span class="sd">    </span>
<span class="sd">    In the Product of Experts (PoE) model, the variance of the </span>
<span class="sd">    posterior distribution is the harmonic mean of the posterior </span>
<span class="sd">    variances of the child experts. The mean of the posterior is a </span>
<span class="sd">    weighted sum of the means of the child experts multiplied by</span>
<span class="sd">    the posterior variance.</span>
<span class="sd">        </span>
<span class="sd">    .. math::</span>

<span class="sd">        σ^{-2}_{PoE} &amp;= \sum_{k=1}^{M} σ^{-2}_k \\\\</span>
<span class="sd">        μ_{PoE} &amp;= σ^2_{PoE} \sum_{k=1}^{M} μ_k σ^{-2}_k</span>

<span class="sd">    where :math:`μ_{PoE}` and :math:`σ^2_{PoE}` are the final posterior</span>
<span class="sd">    mean and variance, :math:`M` is the number of child experts and</span>
<span class="sd">    :math:`μ_k` and :math:`σ^2_k` are the posterior mean and variance</span>
<span class="sd">    for the :math:`k`\ th child.</span>

<span class="sd">    Attributes: See :obj:`GPModel` and :obj:`ParamList`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">children</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialiser.</span>

<span class="sd">        Args:</span>
<span class="sd">            children (Sequence[GPModel]): The experts to combine the</span>
<span class="sd">                opinions of.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">children</span><span class="p">)</span>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="PoEReduction.build_posterior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PoEReduction.build_posterior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_posterior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_chunks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span>
                            <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">joint_var</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">joint_mu</span> <span class="o">=</span> <span class="n">joint_var</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mu</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">joint_mu</span><span class="p">,</span> <span class="n">joint_var</span></div></div>

<div class="viewcode-block" id="gPoEReduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.gPoEReduction">[docs]</a><span class="k">class</span> <span class="nc">gPoEReduction</span><span class="p">(</span><span class="n">Reduction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the predictions of its children using the gPoE model.</span>
<span class="sd">    </span>
<span class="sd">    The generalised Product of Experts (gPoE) model is similar to the</span>
<span class="sd">    PoE model, except that we give each expert a weight. The variance</span>
<span class="sd">    of the posterior distribution is a weighted harmonic mean of the </span>
<span class="sd">    posterior variances of the child experts. The mean of the </span>
<span class="sd">    posterior is a weighted sum of the means of the child experts</span>
<span class="sd">    multiplied by the posterior variance.</span>

<span class="sd">    .. math::</span>

<span class="sd">        σ^{-2}_{gPoE} &amp;= \sum_{k=1}^{M} β_k σ^{-2}_c \\\\</span>
<span class="sd">        μ_{gPoE} &amp;= σ^2_{gPoE} \sum_{k=1}^{M} β_k μ_k σ^{-2}_k</span>

<span class="sd">    where :math:`μ_{gPoE}` and :math:`σ^2_{gPoE}` are the final posterior</span>
<span class="sd">    mean and variance, :math:`M` is the number of child experts and</span>
<span class="sd">    :math:`μ_k` and :math:`σ^2_k` are the posterior mean and variance</span>
<span class="sd">    for the :math:`k`\ th child and :math:`β_k` is the weight of the</span>
<span class="sd">    :math:`k`\ th child.</span>

<span class="sd">    Note that when :math:`\sum_{k} β_k = 1`, the model falls back to</span>
<span class="sd">    the prior outside the range of the data.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">            A function used to calculate the weight of the experts.</span>

<span class="sd">    For other attributes, see `GPModel` and `ParamList`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">children</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialiser.</span>

<span class="sd">        Args:</span>
<span class="sd">            children (Sequence[GPModel]): The experts to combine the</span>
<span class="sd">                opinions of.</span>
<span class="sd">            weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">                A function used to calculate the weight of the experts.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span> <span class="o">=</span> <span class="n">weightfunction</span>
    <span class="c1">#TODO: better weight functions that don&#39;t need X and Y</span>
    <span class="c1">#@tf_method()</span>
    <span class="c1">#@overrides</span>
    <span class="c1">#def build_prior_mean_var(self, test_points, num_latent, full_cov=False):</span>
    <span class="c1">#    &quot;&quot;&quot;A weighted mean of the prior means and variances of the experts.&quot;&quot;&quot;</span>
    <span class="c1">#    if full_cov:</span>
    <span class="c1">#        raise NotImplementedError(&quot;Full covariance matrix not yet&quot;</span>
    <span class="c1">#                &quot;implemented for distributed GPs.&quot;)</span>
    <span class="c1">#    mu, var = zip(*[child.build_prior_mean_var(</span>
    <span class="c1">#                        test_points, num_latent, full_cov</span>
    <span class="c1">#                    ) for child in self.children])</span>
    <span class="c1">#    weight = self.weightfunction(self.children, test_points)</span>
    <span class="c1">#    mu, var, weight = map(lambda x: tf.pack(x, 0), (mu, var, weight))</span>

    <span class="c1">#    w_total = tf.reduce_sum(weight, 0)</span>
    <span class="c1">#    joint_mu = tf.reduce_sum(weight * mu, 0) / w_total</span>
    <span class="c1">#    joint_var = tf.reduce_sum(weight * var, 0) / w_total</span>
    <span class="c1">#    return joint_mu, joint_var</span>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="gPoEReduction.build_posterior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.gPoEReduction.build_posterior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_posterior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_chunks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span>
                            <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

        <span class="n">joint_var</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">joint_mu</span> <span class="o">=</span> <span class="n">joint_var</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">joint_mu</span><span class="p">,</span> <span class="n">joint_var</span></div></div>

<div class="viewcode-block" id="BCMReduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.BCMReduction">[docs]</a><span class="k">class</span> <span class="nc">BCMReduction</span><span class="p">(</span><span class="n">Reduction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the predictions of its children using the BCM model.</span>
<span class="sd">    </span>
<span class="sd">    In the Bayesian Committee Machine (BCM) model, the variance of the </span>
<span class="sd">    posterior distribution is the harmonic mean of the posterior</span>
<span class="sd">    variances of the child experts, with a correction term based on</span>
<span class="sd">    the prior variance. The mean of the posterior is a weighted sum of </span>
<span class="sd">    the means of the child experts multiplied by the posterior variance.</span>
<span class="sd">        </span>
<span class="sd">    .. math::</span>

<span class="sd">        σ^{-2}_{BCM} &amp;= (\sum_{k=1}^{M} σ^{-2}_k)</span>
<span class="sd">                      + (1 - M) σ^{-2}_{\star\star} \\\\</span>
<span class="sd">        μ_{BCM} &amp;= σ^2_{BCM} \sum_{k=1}^{M} μ_k σ^{-2}_k</span>

<span class="sd">    where :math:`μ_{BCM}` and :math:`σ^2_{BCM}` are the final posterior</span>
<span class="sd">    mean and variance, :math:`M` is the number of child experts,</span>
<span class="sd">    :math:`σ^{-2}_{\star\star}` is the prior precision and</span>
<span class="sd">    :math:`μ_k` and :math:`σ^2_k` are the posterior mean and variance</span>
<span class="sd">    for the :math:`k`\ th child.</span>

<span class="sd">    Attributes: See `GPModel` and `ParamList`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">children</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialiser.</span>

<span class="sd">        Args:</span>
<span class="sd">            children (Sequence[GPModel]): The experts to combine the</span>
<span class="sd">                opinions of.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">children</span><span class="p">)</span>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="BCMReduction.build_posterior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.BCMReduction.build_posterior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_posterior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_chunks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span>
                            <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">num_latent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prior_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span><span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">joint_var</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                        <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">))</span> <span class="o">/</span> <span class="n">prior_var</span><span class="p">)</span>
        <span class="n">joint_mu</span> <span class="o">=</span> <span class="n">joint_var</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mu</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">joint_mu</span><span class="p">,</span> <span class="n">joint_var</span></div></div>

<div class="viewcode-block" id="rBCMReduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.rBCMReduction">[docs]</a><span class="k">class</span> <span class="nc">rBCMReduction</span><span class="p">(</span><span class="n">Reduction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Combines the predictions of its children using the rBCM model.</span>
<span class="sd">    </span>
<span class="sd">    In the Bayesian Committee Machine (rBCM) model, the variance of the </span>
<span class="sd">    posterior distribution is the harmonic mean of the posterior</span>
<span class="sd">    variances of the child experts, with a correction term based on</span>
<span class="sd">    the prior variance and the sum of the weights. The mean of the </span>
<span class="sd">    posterior is a weighted sum of the means of the child experts.</span>
<span class="sd">        </span>
<span class="sd">    .. math::</span>

<span class="sd">        σ^{-2}_{rBCM} &amp;= (\sum_{k=1}^{M} β_k σ^{-2}_k)</span>
<span class="sd">                      + (1 - \sum_{k=1}^{M} β_k) σ^{-2}_{\star\star}\\\\</span>
<span class="sd">        μ_{rBCM} &amp;= σ^2_{rBCM} \sum_{k=1}^{M} β_k μ_k σ^{-2}_k</span>

<span class="sd">    where :math:`μ_{rBCM}` and :math:`σ^2_{rBCM}` are the final posterior</span>
<span class="sd">    mean and variance, :math:`M` is the number of child experts,</span>
<span class="sd">    :math:`σ^{-2}_{\star\star}` is the prior precision and</span>
<span class="sd">    :math:`μ_k` and :math:`σ^2_k` are the posterior mean and variance</span>
<span class="sd">    for the :math:`k`\ th child and :math:`β_k` is the weight of the</span>
<span class="sd">    :math:`k`\ th child.</span>

<span class="sd">    The model always falls back to the prior outside of the range of</span>
<span class="sd">    the data.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">            A function used to calculate the weight of the experts.</span>

<span class="sd">    For other attributes, see `GPModel` and `ParamList`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">children</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialiser.</span>

<span class="sd">        Args:</span>
<span class="sd">            children (Sequence[GPModel]): The experts to combine the</span>
<span class="sd">                opinions of.</span>
<span class="sd">            weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">                A function used to calculate the weight of the experts.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">children</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span> <span class="o">=</span> <span class="n">weightfunction</span>

    <span class="c1">#TODO: better weight functions that don&#39;t need X and Y</span>
    <span class="c1">#@tf_method()</span>
    <span class="c1">#@overrides</span>
    <span class="c1">#def build_prior_mean_var(self, test_points, num_latent, full_cov=False):</span>
    <span class="c1">#    &quot;&quot;&quot;A weighted mean of the prior means and variances of the experts.&quot;&quot;&quot;</span>
    <span class="c1">#    if full_cov:</span>
    <span class="c1">#        raise NotImplementedError(&quot;Full covariance matrix not yet&quot;</span>
    <span class="c1">#                &quot;implemented for distributed GPs.&quot;)</span>
    <span class="c1">#    mu, var = zip(*[child.build_prior_mean_var(</span>
    <span class="c1">#                        test_points, num_latent, full_cov</span>
    <span class="c1">#                    ) for child in self.children])</span>
    <span class="c1">#    weight = self.weightfunction(self.children, test_points)</span>
    <span class="c1">#    mu, var, weight = map(lambda x: tf.pack(x, 0), (mu, var, weight))</span>

    <span class="c1">#    w_total = tf.reduce_sum(weight, 0)</span>
    <span class="c1">#    joint_mu = tf.reduce_sum(weight * mu, 0) / w_total</span>
    <span class="c1">#    joint_var = tf.reduce_sum(weight * var, 0) / w_total</span>
    <span class="c1">#    return joint_mu, joint_var</span>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="rBCMReduction.build_posterior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.rBCMReduction.build_posterior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_posterior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_chunks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">child</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span>
                            <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span>
                        <span class="p">)</span> <span class="k">for</span> <span class="n">child</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Yk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

        <span class="n">num_latent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prior_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span><span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">joint_var</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                         <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">prior_var</span><span class="p">)</span>
        <span class="n">joint_mu</span> <span class="o">=</span> <span class="n">joint_var</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">/</span> <span class="n">var</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">joint_mu</span><span class="p">,</span> <span class="n">joint_var</span></div></div>

<div class="viewcode-block" id="PriorDivisorReduction"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PriorDivisorReduction">[docs]</a><span class="k">class</span> <span class="nc">PriorDivisorReduction</span><span class="p">(</span><span class="n">GPModel</span><span class="p">,</span> <span class="n">ParamAttributes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Divides by the prior in proportion to the weight.</span>

<span class="sd">    This is mostly a convenience cljkass for constructing a hierarchical</span>
<span class="sd">    rBCM. It delegates the `.build_log_likelihood()` and </span>
<span class="sd">    `.build_prior_mean_var()` methods to the child expert, and corrects</span>
<span class="sd">    the posterior mean and variance with a prior division term.</span>
<span class="sd">    </span>
<span class="sd">    Attributes:</span>
<span class="sd">        child (GPModel): The expert to correct the opinion of.</span>
<span class="sd">        weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">            A function used to calculate the weight of the expert.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialiser.</span>

<span class="sd">        Args:</span>
<span class="sd">            child (GPModel): The expert to correct the opinion of.</span>
<span class="sd">            weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">                A function used to calculate the weight of the expert.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">child</span> <span class="o">=</span> <span class="n">child</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span> <span class="o">=</span> <span class="n">weightfunction</span>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="PriorDivisorReduction.build_log_likelihood"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PriorDivisorReduction.build_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">build_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">child</span><span class="o">.</span><span class="n">build_log_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> </div>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="PriorDivisorReduction.build_prior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PriorDivisorReduction.build_prior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_prior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span>
                <span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">,</span> <span class="n">full_cov</span><span class="p">)</span></div>

    <span class="nd">@tf_method</span><span class="p">()</span>
    <span class="nd">@overrides</span>
<div class="viewcode-block" id="PriorDivisorReduction.build_posterior_mean_var"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.PriorDivisorReduction.build_posterior_mean_var">[docs]</a>    <span class="k">def</span> <span class="nf">build_posterior_mean_var</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">full_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Full covariance matrix not yet&quot;</span>
                    <span class="s2">&quot;implemented for distributed GPs.&quot;</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">build_posterior_mean_var</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">,</span> <span class="n">full_cov</span>
        <span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weightfunction</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">child</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">num_latent</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">prior_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_prior_mean_var</span><span class="p">(</span><span class="n">test_points</span><span class="p">,</span> <span class="n">num_latent</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">joint_var</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight</span><span class="p">)</span> <span class="o">/</span> <span class="n">prior_var</span><span class="p">)</span>
        <span class="n">joint_mu</span> <span class="o">=</span> <span class="n">joint_var</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">/</span> <span class="n">var</span>
        <span class="k">return</span> <span class="n">joint_mu</span><span class="p">,</span> <span class="n">joint_var</span></div></div>

<div class="viewcode-block" id="chunks"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.chunks">[docs]</a><span class="k">def</span> <span class="nf">chunks</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">iterable</span><span class="p">,</span> <span class="n">padvalue</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterator for equal-sized chunks of an iterable.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (int): The size of each chunk.</span>
<span class="sd">        iterable (Iterable): The iterable to seperate.</span>
<span class="sd">        padvalue: A value to pad the last chunk with if `n` does not</span>
<span class="sd">            evenly divide len(list).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; for chunk in chunks(3, &#39;abcdefg&#39;, &#39;x&#39;):</span>
<span class="sd">        ...     print(chunk)</span>
<span class="sd">        (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)</span>
<span class="sd">        (&#39;d&#39;, &#39;e&#39;, &#39;f&#39;)</span>
<span class="sd">        (&#39;g&#39;, &#39;x&#39;, &#39;x&#39;)</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="p">((</span><span class="nb">iter</span><span class="p">(</span><span class="n">iterable</span><span class="p">),)</span> <span class="o">*</span> <span class="n">n</span><span class="p">),</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">padvalue</span><span class="p">)</span></div>

<div class="viewcode-block" id="tree_rBCM"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.tree_rBCM">[docs]</a><span class="k">def</span> <span class="nf">tree_rBCM</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">,</span> <span class="n">architecture</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An rBCM, expressed as a tree of reductions.</span>

<span class="sd">    Args:</span>
<span class="sd">        experts (GPModel | Sequence[GPModel]): The experts to </span>
<span class="sd">            combine the opinions of. If this is a `GPModel`, then it</span>
<span class="sd">            will be shallow-copied to fill out the architecture.</span>
<span class="sd">            If it is a sequence of `GPModel`\ s, then the architecture</span>
<span class="sd">            will have to have as many nodes in its final layer as</span>
<span class="sd">            there are in the sequence, i.e.</span>

<span class="sd">                len(experts) == reduce(operator.mul,architecture,1)</span>

<span class="sd">        weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">            A function used to calculate the weight of the experts.</span>
<span class="sd">        architecture (Sequence[int]): The branching factors at each</span>
<span class="sd">            layer of the rBCM.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># make sure there are enough experts</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">architecture</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">num_experts</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">architecture</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">GPModel</span><span class="p">):</span>
        <span class="n">experts</span> <span class="o">=</span> <span class="p">(</span><span class="n">experts</span><span class="p">,)</span> <span class="o">*</span> <span class="n">num_experts</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">experts</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_experts</span>

    <span class="n">chunksize</span> <span class="o">=</span> <span class="n">architecture</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="p">[</span><span class="n">gPoEReduction</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">(</span><span class="n">chunksize</span><span class="p">,</span> <span class="n">experts</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">chunksize</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">architecture</span><span class="p">):</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="p">[</span><span class="n">PoEReduction</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">(</span><span class="n">chunksize</span><span class="p">,</span> <span class="n">layer</span><span class="p">)]</span>
            
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">def</span> <span class="nf">total_weight</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weightfunction</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_points</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">PriorDivisorReduction</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">total_weight</span><span class="p">)</span></div>

<div class="viewcode-block" id="distributed_tree_rBCM"><a class="viewcode-back" href="../../public_api.html#gptf.distributed.distributed_tree_rBCM">[docs]</a><span class="k">def</span> <span class="nf">distributed_tree_rBCM</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">,</span> 
        <span class="n">clusterspec</span><span class="p">,</span> <span class="n">worker_job</span><span class="o">=</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span> <span class="n">param_server_job</span><span class="o">=</span><span class="s1">&#39;ps&#39;</span><span class="p">,</span> 
        <span class="n">target_job</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">target_protocol</span><span class="o">=</span><span class="s1">&#39;grpc&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a TreerBCM and distributes its computations.</span>

<span class="sd">    Preconditions:</span>
<span class="sd">        if `experts` is a sequence, the number of worker tasks must</span>
<span class="sd">        evenly divide `len(experts)`.</span>
<span class="sd">    </span>
<span class="sd">    If `experts` is a sequence, the architecture of the `TreerBCM` will</span>
<span class="sd">    be `[n, m]`, where `n` is the number of worker tasks and </span>
<span class="sd">    `m = len(experts) / n`. Each `gPoEReduction` will be pinned to a </span>
<span class="sd">    different worker task.</span>
<span class="sd">    </span>
<span class="sd">    If `experts` is a `GPModel`, the architecture of the `TreerBCM` will </span>
<span class="sd">    be `[n]`, where `n` is the number of worker tasks. `experts` will be</span>
<span class="sd">    copied to fill out the architecture, with each copy being pinned to </span>
<span class="sd">    a different worker task.</span>

<span class="sd">    `Param`\ s will be pinned to parameter server tasks in a round-robin</span>
<span class="sd">    fashion, based on the order they appear in `TreerBCM.params`.</span>

<span class="sd">    Additionally, the `.tf_graph` of the `TreerBCM` will be set to a</span>
<span class="sd">    new graph and the `.tf_session_target` will be set to either the</span>
<span class="sd">    first task of the target job or the first task of the parameter</span>
<span class="sd">    server job.</span>

<span class="sd">    Args:</span>
<span class="sd">        experts (GPModel | Sequence[GPModel]): The experts to </span>
<span class="sd">            combine the opinions of. If this is a `GPModel`, then it</span>
<span class="sd">            will be shallow-copied to fill out the architecture.</span>
<span class="sd">            If it is a sequence of `GPModel`\ s, then the architecture</span>
<span class="sd">            will have as many nodes in its final layer as</span>
<span class="sd">            there are in the sequence.</span>
<span class="sd">        weightfunction (Callable[[Sequence[GPModel], tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor]]): </span>
<span class="sd">            A function used to calculate the weight of the experts.</span>
<span class="sd">        clusterspec (tf.train.ClusterSpec): The cluster to</span>
<span class="sd">            distribute tasks over.</span>
<span class="sd">        worker_job (str): The job to assign computationally</span>
<span class="sd">            expensive tasks to.</span>
<span class="sd">        param_server_job (str): The job to assign paramaters to.</span>
<span class="sd">        target_job (str | None): The job to coordinate other jobs</span>
<span class="sd">            from. This is used to find the `tf_session_target`. </span>
<span class="sd">            If `None`, the first task of `param_server_job`</span>
<span class="sd">            will be used as the session target. Otherwise, the </span>
<span class="sd">            first task of the specified job is used.</span>
<span class="sd">        target_protocol (str): The protocol to use when connecting to</span>
<span class="sd">            the target server. Defaults to &#39;grpc&#39;.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_worker_tasks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusterspec</span><span class="o">.</span><span class="n">job_tasks</span><span class="p">(</span><span class="n">worker_job</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">GPModel</span><span class="p">):</span>
        <span class="n">architecture</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_worker_tasks</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">experts</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_worker_tasks</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">architecture</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_worker_tasks</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">experts</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_worker_tasks</span><span class="p">]</span>

    <span class="n">rBCM</span> <span class="o">=</span> <span class="n">tree_rBCM</span><span class="p">(</span><span class="n">experts</span><span class="p">,</span> <span class="n">weightfunction</span><span class="p">,</span> <span class="n">architecture</span><span class="p">)</span>

    <span class="n">rBCM</span><span class="o">.</span><span class="n">tf_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
    
    <span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">clusterspec</span><span class="o">.</span><span class="n">job_tasks</span><span class="p">(</span><span class="n">target_job</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">target_job</span> 
              <span class="k">else</span> <span class="n">clusterspec</span><span class="o">.</span><span class="n">job_tasks</span><span class="p">(</span><span class="n">parameter_server_job</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">rBCM</span><span class="o">.</span><span class="n">tf_session_target</span> <span class="o">=</span> <span class="s1">&#39;{}://{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target_protocol</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rBCM</span><span class="o">.</span><span class="n">child</span><span class="o">.</span><span class="n">children</span><span class="p">):</span>
        <span class="n">child</span><span class="o">.</span><span class="n">tf_device</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="p">(</span><span class="n">job</span><span class="o">=</span><span class="n">worker_job</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">index</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rBCM</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusterspec</span><span class="o">.</span><span class="n">job_tasks</span><span class="p">(</span><span class="n">param_server_job</span><span class="p">))</span>
        <span class="n">param</span><span class="o">.</span><span class="n">tf_device</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">DeviceSpec</span><span class="p">(</span><span class="n">job</span><span class="o">=</span><span class="n">param_server_job</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rBCM</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Blaine Rogers.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
    </div>

    

    
  </body>
</html>